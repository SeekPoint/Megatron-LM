深入理解 Megatron-LM（3）代码结构

https://zhuanlan.zhihu.com/p/650237820

最近在基于Megatron-LM的代码来训练大语言模型，本人觉得Megatron的代码很具有学习意义，
于是大量参考了网上很多对Megatron代码的解读文章和NVIDA Megatron团队公开发布的2篇论文，
并结合最近Megatron-LM代码库的更新，整理成了这几篇系列文章。
Megatron-LM 代码版本：23.06

之前的系列文章：

简枫：Megatron-LM 近期的改动

简枫：深入理解 Megatron-LM（1）基础知识

简枫：深入理解 Megatron-LM（2）原理介绍

1. 导读
NVIDIA Megatron-LM 是一个基于 PyTorch 的分布式训练框架，用来训练基于Transformer的大型语言模型。
Megatron-LM 综合应用了数据并行（Data Parallelism），
张量并行（Tensor Parallelism）和流水线并行（Pipeline Parallelism）来复现 GPT-3，本系列文章对 Megatron-LM 的源码进行分析并介绍相关技术原理。

2. 程序启动
2.1 分布式启动
启动脚本在 /Megatron-LM/examples/pretrain_gpt_distributed.sh，其利用了torchrun 来启动多个进程。
体代码是 pretrain_gpt.py。

yknote===有出路，可能是pretrain_gpt_distributed_mp.sh

因为 GPUS_PER_NODE 是 8，所以 nproc_per_node 是 8，这样，在本机上就启动了8个进程，每个进程之中含有模型的一部分。
进程的rank 是由 torchrun 自动分配的。
.....

2.2 预训练入口
/Megatron-LM/pretrain_gpt.py 会调用 pretrain 函数进行预训练。
.......
forward_step、model_provider 和 train_valid_test_datasets_provider 三个不同的函数分别为预训练提供不同的功能输入。
....

3. 预训练
pretrain 函数的位置：/Megatron-LM/megatron/training.py

.....

4. 初始化
initialize_megatron 函数位置：/Megatron-LM/megatron/initialize.py
......

4.3 初始化进程组全局变量
因为调用了 mpu.initialize_model_parallel 来设置模型并行，数据并行等各种进程组，
所以我们假定目前进程组都已经设置成功，所以每个 rank 对应的进程都有自己的全局变量。
假定目前有16个GPU，属于两个node，rank 0 ～7 属于第一个节点，rank 8 ～ 15 属于第二个节点。
下面的 gi 指的是第 i 个 GPU。
yknote==以下三张为截图
    21.png
    张量并行组的划分方法

    22.png
    流水线并行组的划分方法

    23.png
    数据并行组的划分方法

可以参考文件：/Megatron-LM/megatron/core/parallel_state.py

    # Intra-layer model parallel group that the current rank belongs to.
    _TENSOR_MODEL_PARALLEL_GROUP = None
    # Inter-layer model parallel group that the current rank belongs to.
    _PIPELINE_MODEL_PARALLEL_GROUP = None
    # Model parallel group (both intra- and pipeline) that the current rank belongs to.
    _MODEL_PARALLEL_GROUP = None
    # Embedding group.
    _EMBEDDING_GROUP = None
    # Position embedding group.
    _POSITION_EMBEDDING_GROUP = None
    # Data parallel group that the current rank belongs to.
    _DATA_PARALLEL_GROUP = None

5. 设置模型
pretrain 函数会调用如下来设置模型，优化器等等。

    # Model, optimizer, and learning rate. 使用model_provider设置模型、优化器和lr-scheduler
    timers('model-and-optimizer-setup', log_level=0).start(barrier=True)
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
            model_provider, model_type)
    timers('model-and-optimizer-setup').stop()
    print_datetime('after model, optimizer, and learning rate '
                       'scheduler are built')
。。。。。


6. 数据并行
...

简单来说：

forward_step() 包含如何做前向的运算
model_provider() 提供了GPTModel
train_valid_test_datastes_provider() 则提供了数据集




参考资料
Reducing Activation Recomputation in Large Transformer Models

Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

[源码解析] 模型并行分布式训练Megatron (1) --- 论文 & 基础

[源码解析] 模型并行分布式训练Megatron (2) --- 整体架构

[细读经典]Megatron论文和代码详细分析(1)

[细读经典]Megatron论文和代码详细分析(2)

编辑于 2023-08-21 23:37・IP 属地浙江