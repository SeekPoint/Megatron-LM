深入理解 Megatron-LM（2）原理介绍
https://zhuanlan.zhihu.com/p/650383289

最近在基于Megatron-LM的代码来训练大语言模型，本人觉得Megatron的代码很具有学习意义，
于是大量参考了网上很多对Megatron代码的解读文章和NVIDA Megatron团队公开发布的2篇论文，
并结合最近Megatron-LM代码库的更新，整理成了这几篇系列文章。
Megatron-LM 代码版本：23.06


简枫：Megatron-LM 近期的改动

简枫：Megatron-LM 源码阅读（1）基础知识

本篇文章对张量模型并行、流水线模型并行的原理进行简单介绍。

1 张量模型并行（Tensor Model Parallelism）
1.1 原理
这里通过 GEMM 来看看如何进行模型并行，这里要进行的是 XA = Y ，对于模型来说，X 是输入，A 是权重，Y 是输出。
从数学原理的角度来看，对于神经网络中的线性层（Linear层），可以将其看作是将输入矩阵分块进行计算，然后将计算结果合并成输出矩阵。
 这个过程涉及矩阵乘法和加法操作，其中矩阵乘法涉及到权重矩阵和输入数据之间的乘法，然后再加上偏置向量。

对于非线性层（例如激活函数层），通常不需要进行额外的设计。
这些层的计算过程是基于输入数据应用某种非线性函数，例如ReLU（修正线性单元）、Sigmoid、Tanh等。
这些函数在数学上是已知的，只需要将输入数据传递给这些函数，然后得到输出。

整体来看，神经网络的计算可以被抽象为一系列的矩阵和向量操作，其中线性层涉及矩阵乘法和加法，而非线性层涉及特定的函数计算。
这些操作在深度学习框架中会被高度优化，以提高计算效率和训练速度。

1.2 行并行（Row Parallelism）
先分析 Row Parallelism，就是把 A 按照行分割成两部分。
为了保证运算，同时也把 X 按照列来分割为两部分，这里X1 的最后一个维度等于 A1最前的一个维度，表示为：

XA = 【X1，X2】【A1 A2】T = X1A1 + X2A2 = Y1 + Y2 = Y   yknote T表示转置--》列向量

所以，X1 和 A1 就可以放到第一个GPU上进行计算，X2 和 A2 可以放到第二个GPU上进行计算，然后把结果相加。
05.png

接下来进行计算。第一步是把图上横向红色箭头和纵向箭头进行点积，得到Y中的绿色方块。
06.png

第三步，计算出来一个新的绿色方块。
07.png

第四步，计算了输出Y的一行。
08.png

第五步，继续执行，得出了一个 Y1
09.png

第六步，得出了蓝色的 Y2 ，此时，可以把 Y1， Y2加起来，得到最终的输出 Y
10.png


1.3 列并行（Column Parallelism）
接下来看看另外一种并行方式 Column Parallelism，就是把 A 按照列来分割。
11.png
最终计算结果如下：
12.png

1.4 Transformer 中的张量并行
自从Google于2018年发布了Attention is All You Need论文，
Transformer模型已经成为自然语言处理领域的核心架构，并被广泛应用于各种NLP任务。

在这种模型架构中，模型的层数与Transformer块的数量一一对应。
每个Transformer块包含了自注意力机制（Self-Attention）和前馈神经网络（Feed Forward Neural Network）。
这些操作确实是大量的矩阵计算，适合在GPU上进行并行操作，从而加速模型的训练和推理过程。
更具体地说，Transformer的每个块包含以下两个主要部分：

    Masked Multi-Head Self Attention：
    这是Transformer中的关键机制，用于建立输入序列中各个位置之间的关系。
    它涉及多个头（head）的自注意力计算，其中每个头会学习不同的上下文关系。
    在计算过程中，涉及到大量的矩阵乘法操作，这些操作可以被高效地并行执行。

    Feed Forward Neural Network：
    这部分也被称为“位置前馈网络”。它包含多个全连接层，每个全连接层都涉及矩阵乘法、激活函数（通常是GeLU）和可能的Dropout层。
    这些操作也是高度并行化的，可以在GPU上迅速执行。

这种设计使得Transformer模型具有良好的可扩展性和计算效率，适用于处理大规模的语言数据。
通过利用GPU的并行计算能力，可以加速模型的训练和推理，使其在处理复杂的自然语言任务时变得更加高效。

具体可以参考下面这篇文章。

简枫：[整理] 聊聊 Transformer

Megatron 的 Feed Forward 是一个两层多层感知器（MLP），第一层是维度从 H 变成 4H，第二层是维度从 4H 变回到 H，
所以Transformer具体架构如下，
紫色块对应于全连接层。每个蓝色块表示一个被复制 N 次的transformer层，红色的 x L 代表此蓝色复制 L 次。
13.png

Transformer 切分

Megatron 就是要把 Masked Multi Self Attention 和Feed Forward 都进行切分以并行化，
利用Transformers网络的结构，通过添加一些同步原语来创建一个简单的模型并行实现。

切分MLP

从MLP块开始。MLP 块的第一部分是GEMM，后面是GeLU：
    Y = GeLU(XA)
并行化GEMM其中的一个选项是沿行方向分割权重矩阵A，沿列切分输入X：
    X = 【X1 X2】 ， A = 【A1  A2】T  yknote T表示转置--》列向量
分区的结果就变成
    Y = GeLU（X1A1 + X2A2）
括号之中的两项，每一个都可以在一个的GPU之上完成，然后通过 all-reduce 操作完成求和。
因为GeLU是一个非线性函数，那么就有
    GeLU（X1A1 + X2A2）不等于 GeLU（X1A1） + GeLU（X2A2）
所以这种方案需要在GeLU函数之前加上一个同步点，这个同步点让不同GPU之间交换信息。

另一个选项是沿列拆分A，得到 A = 【A1  A2】。该分区允许GeLU非线性独立应用于每个分区GEMM的输出。
    【Y1 Y2】= 【 GeLU（XA1） + GeLU（XA2）】

这个方法更好，因为它删除了同步点，直接把两个 GeLU 的输出拼接在一起就行。
因此，以这种列并行方式划分第一个GEMM，并沿其行分割第二个GEMM，
以便它直接获取GeLU层的输出，而不需要任何其他通信（比如 all-reduce 就不需要了），
如下图所示。  14.png

上图第一个是 GeLU 操作，第二个是 Dropout 操作，具体逻辑如下：
1 MLP的整个输入 X 通过 f 放置到每一块 GPU 之上。
2 对于第一个全连接层：
    使用列分割，把权重矩阵切分到两块 GPU 之上，得到A1, A2。
    在每一块 GPU 之上进行矩阵乘法得到第一个全连接层的输出Y1和Y2。
3. 对于第二个全连接层：
    使用行切分，把权重矩阵切分到两个 GPU 之上，得到 B1, B2。
    前面输出Y1和Y2 正好满足需求，直接可以和B的相关部分（B1, B2）做相关计算，不需要通信或者其他操作，就得到了Z1和Z2。
    分别位于两个GPU之上。
4. Z1，Z2通过 g 做 all-reduce（这是一个同步点），再通过 dropout 得到了最终的输出 Z。

然后在 GPU 之上，第二个 GEMM 的输出在传递到 dropout 层之前进行all-reduce。
这种方法将 MLP 块中的两个 GEMM 跨 GPU 进行拆分，
并且只需要在前向过程中进行一次 all-reduce 操作（g 操作符）和在后向过程中进行一次 all-reduce 操作（f 操作符）。

切分self attention
15.png
如上图所示，

    首先，对于Self-Attention块，Megatron 利用了多头attention操作中固有的并行性，
    以列并行方式对与键（K）、查询（Q）和值（V）相关联的GEMM进行分区，从而在一个GPU上本地完成与每个attention头对应的矩阵乘法。
    这使得能够在GPU中分割每个attention head参数和工作负载，每个GPU得到了部分输出。

    其次，对于后续的全连接层，因为每个GPU之上有了部分输出，所以对于权重矩阵B就按行切分，与输入的Y1， Y2。
    进行直接计算，然后通过 g 之中的 all-reduce 操作和Dropout 得到最终结果 Z。

通信成本

来自线性层（在 self attention 层之后）输出的后续 GEMM 会沿着其行去做并行化，并直接获取并行attention层的输出，而不需要GPU之间的通信。
这种用于MLP和Self-Attention层的方法融合了两个GEMM组，消除了中间的同步点，并导致更好的伸缩性。
这使得能够在一个简单的transformer层中执行所有GEMM，只需在正向路径中使用两个all-reduce，在反向路径中使用两个all-reduce（见下图）。
16.png
图：transformer层中的通信操作。在一个单模型并行transformer层的正向和反向传播中总共有4个通信操作。

Transformer语言模型输出了一个Embedding，其维数为隐藏大小（H）乘以词汇量大小（v）。
由于现代语言模型的词汇量约为数万个（例如，GPT-2使用的词汇量为50257），因此将Embedding GEMM的输出并行化是非常有益的。
然而，在transformer语言模型中，想让输出Embedding层与输入Embedding层共享权重，需要对两者进行修改。

Megatron 沿着词汇表维度 E = 【E1， E2】（按列）对输入Embedding权重矩阵 E(H*v下标)进行并行化。
因为每个分区现在只包含Embedding表的一部分，所以在输入Embedding之后需要一个all-reduce（g操作符）。
对于输出Embedding，一种方法是执行并行 GEMM【Y1 Y2】= 【XE1， XE2】以获得logit，
然后添加一个all-gather：Y=allgather（【Y1， Y2】），并将结果发送到交叉熵损失函数。
但是，在这种情况下，由于词汇表很大，all-gather 将传递 b*s*v 个元素（b是batch size，s是序列长度）。
为了减小通信规模，这里将并行 GEMM【Y1 Y2】的输出与交叉熵损失进行融合，从而将维数降低到b*s。

2 流水线模型并行（Pipeline Model Parallelism）
目前主流的流水线并行方法包括了两种：Gpipe和PipeDream。
与这两者相比，Megatron中的流水线并行实现略有不同，它采用了Virtual Pipeline的方法。
简而言之，传统的流水线并行通常会在一个设备上放置几个模块，通过在计算强度和通信强度之间取得平衡来提高效率。
然而，虚拟流水线则采取相反的策略。
在设备数量不变的前提下，它将流水线阶段进一步细分，以承载更多的通信量，从而降低空闲时间的比率，以缩短每个步骤的执行时间。

具体的细节和分析可以阅读下面这篇文章。

Infi-zc：Megatron-LM 中的 pipeline 并行
https://zhuanlan.zhihu.com/p/432969288


3 混合并行设置（Mix Parallelism）
参考的 Megatron 的论文，先对使用的符号做一个说明。
17.png
符号说明

3.1 张量和流水线模型并行 （Tensor and Pipeline Model Parallelism）
张量和流水线模型并行性可以联合作用于多个GPU上来划分模型的参数。
如前所述，将流水线并行性与周期性刷新一起使用会产生大小为（p - 1）/ m的流水线气泡。
假设 d = 1（数据并行大小），因此 t * p = n。在此情况下，流水线气泡大小是：
( p-1 ) / m = ( n/t - 1 ) / m
假如固定B, b 和 d( m = B / (b * d) )也固定下来），当t增加时，流水线气泡会相应减小。

不同GPU之间通信量也受 p 和 t 的影响。流水线模型并行具有开销更小的点对点通信；
另一方面，张量模型并行性使用更消耗带宽的all-reduce通信（向前和向后传递中各有两个all-reduce操作）。

    使用流水线并行，
    在每对连续设备（向前或向后传播）之间为每个微批次（micro-batch）执行的通信总量为b*s*h，s是序列长度，h是隐藏大小（hidden size）。

    使用张量模型并行，
    每个层前向传播和后向传播中，总大小b*s*h的张量需要在t个模型副本之中 all-reduce 两次。

因此，这里看到张量模型并行性增加了设备之间的通信量。当t大于单个节点中的GPU数量时，在较慢的节点间链路上执行张量模型并行是不合算的。

所以，当考虑不同形式的模型并行时，
当使用 g 台-GPU服务器，通常应该把张量模型并行度控制在g之内，然后使用流水线并行来跨服务器扩展到更大的模型。
18.png

3.2 数据和模型并行 （Data and Model Parallelism）
数据并行和流水线并行

给定 t = 1 (tensor-model-parallel size)，那么每个流水线的微批次数目是 m = B / (d * b) = b' / d，
这里 b' = B / b 。给定 GPU 数目为 n ，流水线阶段的数目是 = /(*) = /，流水线气泡大小是：
    ( p-1 ) / m = ( n/t - 1 ) / (b' /d) = (n - d) / b'
当 d 变大，XXX 变小，因此流水线气泡变小。  yknote此处显示不正常
因为模型训练需要的内存占用可能大于单个加速器的内存容量，所以不可能增加  一直到  。
而数据并行性所需的all-reduce通信不会随着更高的数据并行度而增加。

对于 batch size 的大小  ，如批大小  增加， 增加， 会相应减少，从而增加吞吐量。  yknote此处显示不正常
数据并行所需的all-reduce也变得更少，从而进一步提高了吞吐量。

数据并行和张量并行

使用张量模型并行，每个微批次都需要执行all-reduce通信。这在多GPU服务器之间可能非常昂贵。
另一方面，数据并行性对于每个批次只需执行一次 all-reduce。
此外，使用张量模型并行，每个模型并行rank在每个模型层中只执行计算的子集，
因此对于不够大的层，现代GPU可能无法以最高效率执行这些子矩阵计算。

小结

当使用数据和模型并行时，总的模型并行大小应该为 xxx ，这样模型参数和中间元数据可以放入GPU内存。数据并行性可用于将训练扩展到更多GPU。

19.png

3.3 Microbatch Size
Microbatch Size 的值 b 的选择也影响到模型训练的吞吐量。
例如，在单个GPU上，如果Microbatch尺寸较大，每个GPU的吞吐量最多可增加1.3倍。
现在，在定并行配置（p, t, d） 和批量大小 B 下，想确定最佳 Microbatch Size b 。

无论 Microbatch Size 大小如何，数据并行通信量将是相同的。
函数tf(b)和tb(b) 是 Microbatch Size 映射到单个Microbatch 的前向和后向计算时间，
在忽略通信成本的条件下，计算一个batch的总时间为（如前，定义b'为B/d）
    (b'/b + p -1 )*(tf(b) + tb(b))
因此，Microbatch的大小既影响操作的算术强度，也影响管道 bubble 大小（通过影响 m ）。

最佳微批尺寸b取决于模型的吞吐量和内存占用特性，以及管道深度p、数据并行尺寸d 和 批尺寸B。
20.png

4 总结
4.1 Tensor versus Pipeline Parallelism.
在节点内部（如DGX A100服务器），张量模型的并行性表现最佳，因为这可以降低通信量。
另一方面，流水线模型并行性采用更经济的点对点通信方式，可以跨节点执行，而不会受到整个计算的限制。
然而，流水线并行性可能会在流水线“气泡”中消耗大量时间，
因此应该限制流水线的总数，以确保流水线中微批次（micro-batches）的数量是流水线深度的合理倍数。

因此，当张量模型的并行大小等于单个节点中GPU的数量（例如DGX A100有8个GPU的节点）时，性能会达到峰值。
这一结果表明，仅使用张量模型的并行性（如Megatron V1）或仅使用流水线模型的并行性（如PipeDream），
都无法与将这两种技术结合使用时的性能相媲美。

4.2 Pipeline versus Data Parallelism.
通过实验观察发现，针对每个批次大小（batch size），随着流水线并行规模的增加，吞吐量逐渐减少。
因此，流水线模型并行的主要应用场景是支持不适合单个处理单元的大型模型训练，而数据并行则适用于扩大训练规模。

4.3 Tensor versus Data Parallelism.
接下来我们来看看数据并行性和张量模型并行性对性能的影响。在处理较大批次量和微批次为1的情况下，数据并行通信并不频繁；
而张量模型并行则需要对批次中的每个微批次进行全对全（all-to-all）通信。
这种全对全通信在张量模型并行中占据主导地位，对整个端到端的训练时间产生影响，尤其是当通信需要跨多个GPU节点进行时。
此外，随着张量模型并行规模的增加，每个GPU上执行的较小矩阵乘法也降低了每个GPU的利用率。

需要注意的是，虽然数据并行可以有效地扩展训练，但不能仅凭数据并行来处理训练批次受限的大型模型，原因如下：
a）内存容量不足，
b）数据并行的扩展受限（例如，GPT-3的训练批次为1536，因此数据并行仅支持最多1536个GPU的并行化；
然而，该模型的训练涉及约10000个GPU）。

总结
下篇文章介绍Megatron-LM的代码结构。
简枫：深入理解 Megatron-LM（3）代码结构
简枫：深入理解 Megatron-LM（4）并行设置
简枫：深入理解 Megatron-LM（5）张量并行


参考资料
Reducing Activation Recomputation in Large Transformer Models

Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

GTC 2020: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

[源码解析] 模型并行分布式训练Megatron (1) --- 论文 & 基础

[源码解析] 模型并行分布式训练Megatron (2) --- 整体架构

[细读经典]Megatron论文和代码详细分析(1)

[细读经典]Megatron论文和代码详细分析(2)

编辑于 2023-08-22 15:27・IP 属地浙江


发布一条带图评论吧

YangLiu
「另一个选项是沿列拆分A，得到 。该分区允许GeLU非线性独立应用于每个分区GEMM的输出。」
为什么按行拆分不能分别 GeLU，按列拆分可以呢？

作者
YangLiu
客气，我理解按列拆分的话，他们拼接是在最后一个维度上进行的，所以拼接前activate和拼接后activate是等效的。


YangLiu
简枫
我想通了，我的思维误区在于「误认为第一次 GEMM 后的结果 Y1 和 Y2 会拼接」，
但其实每个 device 上第一次 GEMM 后不会拼接，而是依旧 locally 进行第二次 GEMM 后仿射到与原本的 X 的相同的 shape，
之后再进行 All Reduce 通信，获取到其他 device 的梯度后再进行加和。哈哈，再次感谢楼主的文章和回复！


简枫
作者
如果横向拆分A，那么必须要纵向拆分X，并且为了保证XA的完整性，需要X1A1+X2A2。
这样，横向拆分A，必然需要一个同步点，如果这个同步点后面遇到的是线性函数，那当然可以把线性函数放这个同步点的前面。
但是，GeLU 是非线性函数，那么无法先执行这个GeLU，然后再相加，即 GeLU(X1A1)+GeLU(X2A2) != GeLU(X1A1+X2A2）。
所以需要在GeLU之前，设置一个同步点，从而让不同的 gpu 之间交互信息，这会导致一定程度的gpu 等待。


简枫
感谢回复！我明白按行拆分 --> all reduce 这边的分析；
疑惑的是另一边，按列拆分的时候：
两个 sharded results 在（1）拼接前分别 activate --> all reduce，
和（2）拼接后 activate --> all reduce，这两种方式是等效的嘛

