# 3.4 å®éªŒ
# æˆ‘ä»¬æ¥ä¸‹æ¥åšä¸€ä¸ªå®éªŒçœ‹çœ‹ã€‚

import torch

world_size = 16
tensor_model_parallel_size = 2 # 2 GPUs to parallelize the model tensor
pipeline_model_parallel_size = 4 # 4 GPUs to parallelize the model pipeline
data_parallel_size = world_size // (tensor_model_parallel_size *
                                    pipeline_model_parallel_size) # 2
num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size # 8
num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size # 4
num_data_parallel_groups = world_size // data_parallel_size # 8

# Build the data-parallel groups.
print("------ Build the data-parallel groups -----")
all_data_parallel_group_ranks = []
for i in range(pipeline_model_parallel_size):
    start_rank = i * num_pipeline_model_parallel_groups
    end_rank = (i + 1) * num_pipeline_model_parallel_groups
    for j in range(tensor_model_parallel_size):
        ranks = range(start_rank + j, end_rank,
                      tensor_model_parallel_size)
        all_data_parallel_group_ranks.append(list(ranks))
print(all_data_parallel_group_ranks)

# Build the model-parallel groups.
print("------ Build the model-parallel groups -----")
for i in range(data_parallel_size):
    ranks = [data_parallel_group_ranks[i]
             for data_parallel_group_ranks in all_data_parallel_group_ranks]
    print(list(ranks))

# Build the tensor model-parallel groups.
print("------ Build the tensor model-parallel groups -----")
for i in range(num_tensor_model_parallel_groups):
    ranks = range(i * tensor_model_parallel_size,
                  (i + 1) * tensor_model_parallel_size)
    print(list(ranks))

# Build the pipeline model-parallel groups and embedding groups
# (first and last rank in each pipeline model-parallel group).
print("------ Build the pipeline model-parallel groups -----")
for i in range(num_pipeline_model_parallel_groups):
    ranks = range(i, world_size,
                  num_pipeline_model_parallel_groups)
    print(list(ranks))
# è¾“å‡ºå¦‚ä¸‹ã€‚éœ€è¦æ³¨æ„ï¼Œè¿™é‡Œéƒ½æ˜¯ GPU çš„åºåˆ—å·ï¼Œ[0,2] å°±æ˜¯ [g0, g2]ï¼š
#
# ------ Build the data-parallel groups -----
# [[0, 2], [1, 3], [4, 6], [5, 7], [8, 10], [9, 11], [12, 14], [13, 15]]
# ------ Build the model-parallel groups -----
# [0, 1, 4, 5, 8, 9, 12, 13]
# [2, 3, 6, 7, 10, 11, 14, 15]
# ------ Build the tensor model-parallel groups -----
# [0, 1]
# [2, 3]
# [4, 5]
# [6, 7]
# [8, 9]
# [10, 11]
# [12, 13]
# [14, 15]
# ------ Build the pipeline model-parallel groups -----
# [0, 4, 8, 12]
# [1, 5, 9, 13]
# [2, 6, 10, 14]
# [3, 7, 11, 15]
#
# æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹æ³¨é‡Šï¼Œå‘ç°ä»£ç æ‰“å°ç»“æœå¯ä»¥å’Œæ³¨é‡Šå¯¹åº”ä¸Šï¼š
#     Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we
#     use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
#     the model pipeline. The present function will
#     create 8 tensor model-parallel groups, 4 pipeline model-parallel groups
#     and 8 data-parallel groups as:
#         8 data_parallel groups:
#             [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
#         8 tensor model-parallel groups:
#             [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
#         4 pipeline model-parallel groups:
#             [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
# æˆ‘ä»¬æ¥ä¸‹æ¥ä¼šè¿›è¡Œå…·ä½“åˆ†æã€‚


'''
0x04 èµ·å§‹çŠ¶æ€
4.1 GPU çŠ¶å†µ
ä»æ³¨é‡Šä¸­å¯ä»¥çœ‹åˆ°ï¼š

Note that for efficiency, the caller should make sure adjacent ranks are on the same DGX box. For example if we are using 2 DGX-1 boxes with a total of 16 GPUs, rank 0 to 7 belong to the first box and ranks 8 to 15 belong to the second box.
æ„æ€å°±æ˜¯ï¼šè°ƒç”¨è€…éœ€è¦ç¡®ä¿ç›¸é‚»çš„rankåœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬ä¾‹å­æœ‰ä¸¤ä¸ªNodeï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªNodeæ‹¥æœ‰ GPU 0 ï½ 7ï¼Œå°±æ˜¯ rank 0 ï½ 7ï¼Œç¬¬äºŒä¸ªNodeæ˜¯ GPU 8ï½15ï¼Œå°±æ˜¯ rank 8 ï½ 15ã€‚

å…·ä½“å¦‚ä¸‹ï¼Œè¿™é‡Œæ¯è¡Œ4ä¸ªGPUï¼Œæ˜¯å› ä¸º 4 GPUs to parallelize the model pipelineï¼Œæ‰€ä»¥æµæ°´çº¿æ¯ä¸ªstageæ˜¯4ä¸ªGPUã€‚



4.2 ç¬¦å·è¯´æ˜
ä¸‹é¢æ˜¯è®ºæ–‡ä¹‹ä¸­æåˆ°çš„ä¸€äº›ç¬¦å·ï¼Œè¿™é‡Œæœ‰å¿…è¦å†å–å‡ºæ¥æ¸©ä¹ ä¸€ä¸‹ï¼š

(ğ‘, ğ‘¡, ğ‘‘): Parallelization dimensions.

ğ‘ for the pipeline-modelparallel size,

ğ‘¡ for the tensor-model-parallel size, and ğ‘‘ for the data-parallel size.

ğ‘›: Number of GPUs. We require ğ‘ Â· ğ‘¡ Â· ğ‘‘ = ğ‘›.

4.3 åˆå§‹åˆ†ç»„
ä¾æ®æ³¨é‡Šï¼Œæˆ‘ä»¬å¾—å‡ºç›®å‰åˆ†ç»„æƒ…å†µå’Œä¸€äº›å…¨å±€ä¿¡æ¯ã€‚

ä¸€å…±16ä¸ªGPUï¼Œæ‰€ä»¥ world_size ä¸º 16ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ nã€‚
ä½¿ç”¨ä¸¤ä¸ªGPUè¿›è¡Œ model tensor å¹¶è¡Œï¼Œæ‰€ä»¥ tensor_model_parallel_size = 2ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ tã€‚
ä½¿ç”¨å››ä¸ªGPUè¿›è¡Œæ¨¡å‹æµæ°´çº¿å¹¶è¡Œï¼Œæ‰€ä»¥ pipeline_model_parallel_size = 4ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ pã€‚å…¶å®ï¼Œå°±æ˜¯æµæ°´çº¿æ·±åº¦ä¸º 4ï¼Œå³ï¼Œ4 ä¸ª GPU æ˜¯ä¸²è¡Œçš„ã€‚
ä¾æ®ä¸Šé¢å®šä¹‰ï¼Œd = n / ( t * p) = 2ï¼Œå°±æ˜¯ data_parallel_size = 2ã€‚å› ä¸º t * p å°±æ˜¯ä¸€ä¸ªæ¨¡å‹æ‰€éœ€è¦çš„ GPUï¼Œd = (æ€» GPU / ä¸€ä¸ªæ¨¡å‹éœ€è¦çš„ GPU)ï¼Œç»“æœæ˜¯è¿™äº›GPUå¯ä»¥è®­ç»ƒ d ä¸ªæ¨¡å‹ï¼Œå°±æ˜¯å¯ä»¥ç”¨ d ä¸ª mini-batches è¿›è¡Œè¿™ä¸ª dä¸ªæ¨¡å‹ä¸€èµ·è®­ç»ƒï¼Œæ‰€ä»¥æ•°æ®å¹¶è¡Œåº¦ä¸º dã€‚
æ¥ä¸‹æ¥ç»“åˆä»£ç çœ‹çœ‹éœ€è¦åˆ†æˆå¤šå°‘ä¸ªprocess groupsï¼Œä»–ä»¬åœ¨ä»£ç ä¹‹ä¸­çš„å˜é‡æ˜¯ä»€ä¹ˆã€‚

num_tensor_model_parallel_groups å°±æ˜¯ä» tensor model å¹¶è¡Œè§’åº¦çœ‹ï¼Œåˆ†æˆ8 ä¸ªè¿›ç¨‹roupã€‚
num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size å°±æ˜¯ä» model å¹¶è¡Œè§’åº¦çœ‹ï¼Œåˆ†æˆ 4 ä¸ª è¿›ç¨‹groupã€‚
num_data_parallel_groups = world_size // data_parallel_size å°±æ˜¯ä»data å¹¶è¡Œè§’åº¦çœ‹ï¼Œåˆ†æˆ8 ä¸ª è¿›ç¨‹groupã€‚å°±æ˜¯ä¼šæœ‰ 8 ä¸ª DDPï¼Œæ¯ä¸ª DDP åŒ…æ‹¬ 2 ä¸ª rankã€‚
è¿˜æœ‰ä¸€ä¸ª _MODEL_PARALLEL_GROUPï¼Œ
å…·ä½“å¦‚ä¸‹ï¼š

world_size = 16
tensor_model_parallel_size = 2 # 2 GPUs to parallelize the model tensor
pipeline_model_parallel_size = 4 # 4 GPUs to parallelize the model pipeline
data_parallel_size = world_size // (tensor_model_parallel_size *
                                    pipeline_model_parallel_size) # 2
num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size # 8
num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size # 4
num_data_parallel_groups = world_size // data_parallel_size # 8
0x05 Tensor model-parallel
æœ¬èŠ‚æˆ‘ä»¬åˆ†æçš„æ˜¯ï¼Œå¦‚ä½•å°† Node ä¸Šçš„ GPU åˆ†ç»™ tensor model å¹¶è¡Œç»„ã€‚

5.1 åˆ†ç»„
å¯¹äºæ³¨é‡Šä¾‹å­ï¼Œ16 / 2 = 8ï¼Œåˆ†æˆ 8 ä¸ªè¿›ç¨‹ç»„ï¼Œæ¯ä¸ªç»„ ä¸¤ä¸ª rankã€‚è¿™äº›åˆ†ç»„åˆ†åˆ«æ˜¯ï¼š[g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹ä¿¡æ¯ï¼š

[g0, g1] å°±æ˜¯æŸä¸€å±‚åˆ†åˆ‡ä¸º2åŠï¼Œåˆ†åˆ«è¢« g0, g1 æ¥æ‰§è¡Œï¼Œ[g2, g3] è¡¨ç¤ºå¦ä¸€å±‚è¢«åˆ†ä¸ºä¸¤å±‚ï¼Œåˆ†åˆ«è¢« g2ï¼Œg3 æ¥æ‰§è¡Œã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€ä¸ª tensor-model-parallel groupçš„ rankä¸€å®šæ˜¯ç›¸é‚»çš„ï¼Œæ¯”å¦‚ [g0, g1], [g2, g3]ã€‚

æ³¨æ„ï¼Œ0 ~ 7 ä¸ä»£è¡¨æ˜¯åŒä¸€ä¸ªæ¨¡å‹ã€‚0 ~ 7 æ˜¯åŒä¸€ä¸ª Node ä¸Šçš„ GPUï¼Œè¿™ç‚¹å®¹æ˜“è¢«æ··æ·†ã€‚


'''