æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ
https://www.cnblogs.com/rossiXYZ/p/15876714.html

ç›®å½•
[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (4) --- å¦‚ä½•è®¾ç½®å„ç§å¹¶è¡Œ
0x00 æ‘˜è¦
0x01 å‰æ–‡å›é¡¾
0x02 åˆå§‹åŒ–
2.1 å…¨å±€å˜é‡
2.2 åˆå§‹åŒ–ä»£ç 
0x03 åˆ‡åˆ†æ ·ä¾‹
3.1 æ³¨é‡Š
3.2 åˆ‡åˆ†æƒ…å†µ
3.3 åˆ‡åˆ†ç­–ç•¥
3.4 å®éªŒ
0x04 èµ·å§‹çŠ¶æ€
4.1 GPU çŠ¶å†µ
4.2 ç¬¦å·è¯´æ˜
4.3 åˆå§‹åˆ†ç»„
0x05 Tensor model-parallel
5.1 åˆ†ç»„
5.2 ä½¿ç”¨
0x06 Pipe-parallel
6.1 åˆ†ç»„
6.2 ä½¿ç”¨
6.2.1 ä¸Šä¸‹æ¸¸rank
6.2.2 world size
0x07 Data-parallel
7.1 åˆ†ç»„
7.2 å¦‚ä½•ä½¿ç”¨
0x08 æ¨¡å‹ç»„
0x09 å¦‚ä½•æŠŠæ¨¡å‹åˆ†åˆ°GPU
0xFF å‚è€ƒ

0x00 æ‘˜è¦
NVIDIA Megatron æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œç”¨æ¥è®­ç»ƒè¶…å¤§Transformerè¯­è¨€æ¨¡å‹ï¼Œ
å…¶é€šè¿‡ç»¼åˆåº”ç”¨äº†æ•°æ®å¹¶è¡Œï¼ŒTensorå¹¶è¡Œå’ŒPipelineå¹¶è¡Œæ¥å¤ç° GPT3ï¼Œå€¼å¾—æˆ‘ä»¬æ·±å…¥åˆ†æå…¶èƒŒåæœºç†ã€‚

æœ¬ç³»åˆ—å¤§æ¦‚æœ‰ 5 ç¯‡æ–‡ç« ï¼Œé€šè¿‡è®ºæ–‡å’Œæºç å’Œå¤§å®¶ä¸€èµ·å­¦ä¹ ç ”ç©¶ã€‚æœ¬æ–‡å°†çœ‹çœ‹ Megatron å¦‚ä½•å¤„ç†è®¾ç½®å¹¶è¡Œã€‚

æœ¬ç³»åˆ—å…¶ä»–æ–‡ç« ä¸ºï¼š

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (1) --- è®ºæ–‡ & åŸºç¡€

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (2) --- æ•´ä½“æ¶æ„

[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ Megatron (3) ---æ¨¡å‹å¹¶è¡Œå®ç°


0x01 å‰æ–‡å›é¡¾
å‰æ–‡æˆ‘ä»¬å¯¹æ¨¡å‹å¹¶è¡Œçš„åŸç†å’Œä»£ç è¿›è¡Œäº†åˆ†æï¼Œå¯¹äºç»™å®šçš„æ¨¡å‹ï¼Œç°åœ¨è¿˜éœ€è¦è§£å†³å‡ ä¸ªé—®é¢˜ï¼š
    å¦‚ä½•æŠŠæ¨¡å‹åˆ‡åˆ†ç»™èŠ‚ç‚¹ï¼Œæ¯”å¦‚å“ªä¸ªèŠ‚ç‚¹è´Ÿè´£å“ªäº›å±‚ã€‚
    æ•°æ®å¹¶è¡Œï¼Œæ¨¡å‹å¹¶è¡Œï¼Œæµæ°´çº¿å¹¶è¡Œè¿™å‡ ç§å¹¶è¡Œä¹‹ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹åˆ†åˆ«å±äºå“ªä¸ªéƒ¨åˆ†ï¼Ÿ
    å¦‚ä½•é¿å…æµæ°´çº¿å¸¦æ¥çš„é—®é¢˜ã€‚
æˆ‘ä»¬æ¥ä¸‹æ¥å°±ä»”ç»†åˆ†æä¸€ä¸‹ã€‚

0x02 åˆå§‹åŒ–
initialize_model_parallel æ–¹æ³•ç”¨æ¥è®¾ç½®æ¨¡å‹å¹¶è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥ä¸‹æ¥å°±å…·ä½“åˆ†æã€‚

2.1 å…¨å±€å˜é‡
å› ä¸ºå‰æ–‡_initialize_distributedä¹‹ä¸­è°ƒç”¨äº†torch.distributed.init_process_group åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼Œ
æ‰€ä»¥æˆ‘ä»¬çŸ¥é“ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„ gloabl rank å’Œ local rankï¼Œéƒ½æœ‰è‡ªå·±çš„å…¨å±€å˜é‡ã€‚

ä¸»è¦å˜é‡å¦‚ä¸‹ï¼ˆå…·ä½“ä¾‹å­å¯ä»¥ç»“åˆ initialize_model_parallel ä¹‹ä¸­çš„æ³¨é‡Šæ¥çœ‹ï¼‰ï¼š

    _TENSOR_MODEL_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„Intra-layer model parallel groupï¼Œå°±æ˜¯tensor å¹¶è¡Œè¿›ç¨‹ç»„ã€‚
        å‡å¦‚æ¯ä¸€å±‚åˆ†ä¸ºä¸¤ä¸ªtensorï¼Œåˆ™ _TENSOR_MODEL_PARALLEL_GROUP ä¾‹å­ä¸ºï¼š[g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]ã€‚
    _PIPELINE_MODEL_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„Intra-layer model parallel groupï¼Œå°±æ˜¯æµæ°´çº¿è¿›ç¨‹ç»„ã€‚
        å‡å¦‚æµæ°´çº¿æ·±åº¦ä¸º4ï¼Œåˆ™ä¾‹å­ä¸º [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]ã€‚
    _MODEL_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„æ¨¡å‹å¹¶è¡Œè¿›ç¨‹ç»„ï¼ŒåŒ…æ‹¬äº†ä»¥ä¸Šä¸¤ç»„ã€‚
        é’ˆå¯¹æˆ‘ä»¬ä¾‹å­ï¼Œå°±æ˜¯å®Œæ•´æ¨¡å‹è¢«å¤åˆ¶äº†ä¸¤ä»½ï¼Œå…¶ GPU èŠ‚ç‚¹å…·ä½“æ˜¯[0, 1, 4, 5, 8, 9, 12, 13]ï¼Œ[2, 3, 6, 7, 10, 11, 14, 15]
    _EMBEDDING_GROUP ï¼š åµŒå…¥å¯¹åº”çš„è¿›ç¨‹ç»„ã€‚
    _DATA_PARALLEL_GROUP ï¼šå½“å‰ rank æ‰€å±äºçš„Data parallel groupã€‚
        å‡å¦‚æ•°æ®å¹¶è¡Œåº¦æ•°ä¸º2ï¼Œåˆ™ä¾‹å­ä¸º[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]ã€‚

å…·ä½“å¦‚ä¸‹ï¼š

    # Intra-layer model parallel group that the current rank belongs to.
    _TENSOR_MODEL_PARALLEL_GROUP = None
    # Inter-layer model parallel group that the current rank belongs to.
    _PIPELINE_MODEL_PARALLEL_GROUP = None
    # Model parallel group (both intra- and pipeline) that the current rank belongs to.
    _MODEL_PARALLEL_GROUP = None
    # Embedding group.
    _EMBEDDING_GROUP = None
    # Data parallel group that the current rank belongs to.
    _DATA_PARALLEL_GROUP = None

    _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = None
    _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None
    _PIPELINE_MODEL_PARALLEL_SPLIT_RANK = None

    # These values enable us to change the mpu sizes on the fly.
    _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE = None
    _MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None
    _MPU_TENSOR_MODEL_PARALLEL_RANK = None
    _MPU_PIPELINE_MODEL_PARALLEL_RANK = None

    # A list of ranks that have a copy of the embedding.
    _EMBEDDING_GLOBAL_RANKS = None

    # A list of global ranks for each pipeline group to ease calculation of the source
    # rank when broadcasting from the first or last pipeline stage.
    _PIPELINE_GLOBAL_RANKS = None


0x03 åˆ‡åˆ†æ ·ä¾‹
æˆ‘ä»¬ä½¿ç”¨æ³¨é‡Šå†…å®¹æ¥è¿›è¡Œå­¦ä¹ å¦‚ä½•åˆ‡åˆ†æ¨¡å‹ï¼Œå¦‚ä½•æŠŠå¤šç§å¹¶è¡Œæ¨¡å¼ç»„åˆåœ¨ä¸€èµ·ã€‚

3.1 æ³¨é‡Š
initialize_model_parallel çš„æ³¨é‡Šå€¼å¾—æˆ‘ä»¬æ·±å…¥å­¦ä¹ ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

    Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we
    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
    the model pipeline. The present function will
    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups
    and 8 data-parallel groups as:
        8 data_parallel groups:
            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
        8 tensor model-parallel groups:
            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
        4 pipeline model-parallel groups:
            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
    Note that for efficiency, the caller should make sure adjacent ranks
    are on the same DGX box. For example if we are using 2 DGX-1 boxes
    with a total of 16 GPUs, rank 0 to 7 belong to the first box and
    ranks 8 to 15 belong to the second box.

ä»æ³¨é‡Šå¯ä»¥çŸ¥é“å¦‚ä¸‹ä¿¡æ¯ï¼š

    å‡å®šç›®å‰æœ‰16ä¸ªGPUï¼Œå±äºä¸¤ä¸ªnodeï¼Œrank 0 ï½7 å±äºç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œrank 8 ï½ 15 å±äºç¬¬äºŒä¸ªèŠ‚ç‚¹ã€‚

    create 8 tensor model-parallel groups, 4 pipeline model-parallel groupsï¼Œè¿™è¯´æ˜å°†ä¸€ä¸ªå®Œæ•´æ¨¡å‹åˆ‡åˆ†å¦‚ä¸‹ï¼š

        æ²¿ç€è¡Œæ¨ªå‘åˆ‡äº†ä¸€åˆ€ï¼š
        tensor_model_parallel_size = 16 / 8 = 2ï¼Œå°±æ˜¯2ä¸ª GPUs æ¥è¿›è¡Œæ¨¡å‹å¼ é‡å¹¶è¡Œã€‚

        æ²¿ç€åˆ—çºµå‘åˆ‡äº†ä¸‰åˆ€ï¼š
        pipeline_model_parallel_size = 16 /4 = 4ï¼Œå°±æ˜¯4ä¸ªGPUs è¿›è¡Œæµæ°´çº¿å¹¶è¡Œã€‚

        å› æ­¤ï¼Œä¸€ä¸ªæ¨¡å‹åˆ†ä¸º8å—ï¼Œæ¯ä¸€å—æ”¾åœ¨ä¸€ä¸ªGPUä¹‹ä¸Šï¼Œå°±æ˜¯8ä¸ªGPUã€‚
        è€Œé€šè¿‡å¦‚ä¸‹è®¡ç®—å¯ä»¥çŸ¥ 16 GPUs / 8 GPUs = 2 modelsã€‚å³ï¼Œ16å¼ å¡å¯ä»¥æ”¾ç½®ä¸¤ä¸ªå®Œæ•´æ¨¡å‹ã€‚

    å› ä¸ºå¼ é‡æ¨¡å‹å¹¶è¡Œç»„å¤§å°æ˜¯2ï¼Œå³16ä¸ªGPUè¢«åˆ†æˆ8ç»„ï¼Œ
    åˆ™è¿™8ç»„å†…å®¹æ˜¯ [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]ã€‚

    å› ä¸ºæµæ°´çº¿å¹¶è¡Œç»„å¤§å°æ˜¯4ï¼Œå³16ä¸ªGPUè¢«åˆ†æˆ4ç»„ï¼Œ
    åˆ™è¿™4ç»„å†…å®¹æ˜¯[g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]ã€‚

    å› ä¸ºæ•°æ®å¹¶è¡Œç»„å¤§å°æ˜¯2ï¼Œ16ä¸ªGPUè¢«åˆ†æˆ8ç»„ï¼Œ
    åˆ™è¿™8ç»„å†…å®¹æ˜¯[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]ã€‚

    ä»¥ä¸Šè¿™äº›è¿›ç¨‹ç»„éƒ½æ˜¯é€šè¿‡ torch.distributed.new_group æ¥å®Œæˆï¼Œ
    è¿™æ ·ç»„å†…è¿›ç¨‹ä¹‹é—´å°±çŸ¥é“å“ªäº›è¿›ç¨‹æ˜¯åœ¨åŒä¸€ä¸ªç»„å†…ï¼Œæ˜¯åœ¨ä¸€èµ·è®­ç»ƒçš„ï¼Œä¹ŸçŸ¥é“æ€ä¹ˆé€šä¿¡ã€‚



3.2 åˆ‡åˆ†æƒ…å†µ
æ¨¡å‹åŸå§‹å›¾å¦‚ä¸‹ 50.png
æ¨¡å‹åˆ‡åˆ†ä¹‹åå¦‚ä¸‹ï¼Œä¸€å…±è¢«åˆ†æˆ8å—ã€‚å…¶ä¸­ï¼Œç¬¬ä¸€å±‚è¢«åˆ‡åˆ†ä¸º Aï¼ŒBï¼Œæ‰€ä»¥ Aï¼ŒB ä¹‹é—´å°±æ˜¯ Tensor Model parallelã€‚
åé¢ Cï¼ŒD ä¹‹é—´ä¹Ÿæ˜¯ Tensor Model parallelï¼ŒæŠŠä¸¤å±‚éƒ½åšäº†åˆ‡åˆ†ï¼Œä¾æ¬¡ç±»æ¨ã€‚
51.png

æˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯ç”¨ä»£ç æ¥çœ‹çœ‹å¦‚ä½•ç”Ÿæˆæ³¨é‡Šé‡Œé¢çš„å„ç§æ¨¡å‹ç»„ã€‚

3.3 åˆ‡åˆ†ç­–ç•¥
æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å…·ä½“åˆ‡åˆ†çš„ç­–ç•¥ï¼Œä¹Ÿå°±æ˜¯GPUåˆ†é…ç­–ç•¥ã€‚åˆ‡åˆ†éœ€è¦ç»¼åˆè€ƒè™‘å¤šç§æƒ…å†µï¼Œé¦–å…ˆçœ‹çœ‹æ¨¡å‹å¹¶è¡Œçš„é€šä¿¡çŠ¶å†µã€‚

    å¼ é‡å¹¶è¡Œï¼š
    é€šä¿¡å‘ç”Ÿåœ¨æ¯å±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¹‹ä¸­ï¼Œé€šä¿¡ç±»å‹æ˜¯all-reduceï¼Œä¸ä½†å•æ¬¡é€šä¿¡æ•°æ®é‡å¤§ï¼Œå¹¶ä¸”é€šä¿¡é¢‘ç¹ã€‚

    æµæ°´çº¿å¹¶è¡Œï¼š
    é€šä¿¡åœ¨æµæ°´çº¿é˜¶æ®µç›¸é‚»çš„åˆ‡åˆ†ç‚¹ä¹‹ä¸Šï¼Œé€šä¿¡ç±»å‹æ˜¯P2Pé€šä¿¡ï¼Œå•è¯é€šä¿¡æ•°æ®é‡è¾ƒå°‘ä½†æ˜¯æ¯”è¾ƒé¢‘ç¹ï¼Œ
    è€Œä¸”å› ä¸ºæµæ°´çº¿çš„ç‰¹ç‚¹ï¼Œä¼šäº§ç”ŸGPUç©ºé—²æ—¶é—´ï¼Œè¿™é‡Œç§°ä¸ºæµæ°´çº¿æ°”æ³¡ï¼ˆBubbleï¼‰ã€‚

æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å„ç§å¹¶è¡Œæœºåˆ¶çš„å¯¹æ¯”ã€‚

    Tensor versus Pipeline Parallelism.
    å¼ é‡æ¨¡å‹çš„å¹¶è¡Œæ€§åœ¨èŠ‚ç‚¹å†…æ˜¯æœ€å¥½çš„ï¼Œå› ä¸ºå®ƒä¼šå‡å°‘é€šä¿¡é‡ã€‚
    å¦ä¸€æ–¹é¢ï¼Œæµæ°´çº¿æ¨¡å‹å¹¶è¡Œä½¿ç”¨æ›´ä¾¿å®œçš„ç‚¹å¯¹ç‚¹é€šä¿¡ï¼Œå¯ä»¥è·¨èŠ‚ç‚¹æ‰§è¡Œï¼Œè€Œä¸ä¼šé™åˆ¶æ•´ä¸ªè®¡ç®—ã€‚
    ç„¶è€Œï¼Œæµæ°´çº¿å¹¶è¡Œæ€§ä¼šåœ¨æµæ°´çº¿æ°”æ³¡ä¸­èŠ±è´¹å¤§é‡æ—¶é—´ï¼Œå› æ­¤ï¼Œåº”é™åˆ¶æµæ°´çº¿çº§çš„æ€»æ•°ï¼Œä»¥ä¾¿æµæ°´çº¿ä¸­çš„microbatchesæ•°é‡æ˜¯æµæ°´çº¿æ·±åº¦çš„åˆç†å€æ•°ã€‚
    å½“å¼ é‡å¹¶è¡Œå¤§å°ç­‰äºå•ä¸ªèŠ‚ç‚¹ä¸­çš„GPUæ•°é‡æ—¶ä¼šè¾¾åˆ°å³°å€¼æ€§èƒ½ã€‚

    Pipeline versus Data Parallelism.
    å¯¹äºæ¯ä¸ªbatch sizeï¼Œååé‡éšç€æµæ°´çº¿å¹¶è¡Œè§„æ¨¡çš„å¢åŠ è€Œé™ä½ã€‚
    æµæ°´çº¿æ¨¡å‹å¹¶è¡Œåº”è¯¥ä¸»è¦ç”¨äºæ”¯æŒä¸é€‚åˆå•ä¸ª worker çš„å¤§å‹æ¨¡å‹è®­ç»ƒã€‚è€Œæ•°æ®å¹¶è¡Œåº”è¯¥ç”¨äºæ‰©å¤§è®­ç»ƒè§„æ¨¡ã€‚

    Tensor versus Data Parallelism.
    æ¥ä¸‹æ¥çœ‹çœ‹æ•°æ®å’Œå¼ é‡æ¨¡å‹çš„å¹¶è¡Œæ€§å¯¹æ€§èƒ½çš„å½±å“ã€‚
    åœ¨è¾ƒå¤§çš„æ‰¹å¤„ç†é‡å’Œå¾®æ‰¹å¤„ç†é‡ä¸º1çš„æƒ…å†µä¸‹ï¼Œæ•°æ®å¹¶è¡Œé€šä¿¡å¹¶ä¸é¢‘ç¹ï¼›å¼ é‡æ¨¡å‹å¹¶è¡Œéœ€è¦å¯¹æ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªå¾®æ‰¹è¿›è¡Œall-to-allé€šä¿¡ã€‚
    è¿™ç§all-to-allçš„é€šä¿¡ä¸»å¯¼äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒæ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å½“é€šä¿¡éœ€è¦åœ¨å¤šGPUèŠ‚ç‚¹ä¸Šè¿›è¡Œæ—¶ã€‚
    æ­¤å¤–ï¼Œéšç€å¼ é‡æ¨¡å‹å¹¶è¡Œè§„æ¨¡çš„å¢åŠ ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªGPUä¸Šæ‰§è¡Œè¾ƒå°çš„çŸ©é˜µä¹˜æ³•ï¼ˆå› ä¸ºä¼šæŠŠæ¨¡å‹å¼ é‡è¿›è¡Œåˆ‡åˆ†ï¼‰ï¼Œè¿™é™ä½äº†æ¯ä¸ªGPUçš„åˆ©ç”¨ç‡ã€‚

æœ€åçœ‹çœ‹ç»“è®º

    Tensoræ¨¡å‹å¹¶è¡Œè¢«ç”¨äºintra-node transformer å±‚ï¼Œ
    å› ä¸ºå¼ é‡å¹¶è¡Œè®¡ç®—å¯†é›†ä¸”æ˜¯è€—è´¹å¤§é‡å¸¦å®½ï¼Œè¿™æ ·ä¼šåœ¨HGX basedç³»ç»Ÿä¸Šé«˜æ•ˆè¿è¡Œã€‚

    Pipeline æ¨¡å‹å¹¶è¡Œä¸»è¦è¢«ç”¨äºinter-node transformer å±‚ï¼Œ
    å› ä¸ºPipeline å¹¶è¡Œçš„é€šä¿¡å¸¦å®½å ç”¨å°‘ï¼Œå…¶å¯ä»¥æœ‰æ•ˆåˆ©ç”¨é›†ç¾¤ä¸­å¤šç½‘å¡è®¾è®¡ã€‚

    æ•°æ®å¹¶è¡Œåˆ™åœ¨å‰ä¸¤è€…åŸºç¡€ä¹‹ä¸Šè¿›è¡ŒåŠ æŒï¼Œä½¿å¾—è®­ç»ƒå¯ä»¥æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡å’Œæ›´å¿«çš„é€Ÿåº¦ã€‚
    æˆ‘ä»¬åº”è¯¥æ³¨æ„åˆ°ï¼Œå°½ç®¡æ•°æ®å¹¶è¡Œå¯ä»¥å¸¦æ¥é«˜æ•ˆçš„æ‰©å±•ï¼Œä½†æˆ‘ä»¬ä¸èƒ½å•ç‹¬ä½¿ç”¨æ•°æ®å¹¶è¡Œæ¥å¤„ç†è®­ç»ƒè¶…å¤§æ¨¡å‹ï¼Œå› ä¸º aï¼‰å†…å­˜å®¹é‡ä¸è¶³ï¼Œbï¼‰æ•°æ®å¹¶è¡Œçš„æ‰©å±•é™åˆ¶ã€‚

3.4 å®éªŒ
æˆ‘ä»¬æ¥ä¸‹æ¥åšä¸€ä¸ªå®éªŒçœ‹çœ‹ã€‚

import torch

world_size = 16
tensor_model_parallel_size = 2 # 2 GPUs to parallelize the model tensor
pipeline_model_parallel_size = 4 # 4 GPUs to parallelize the model pipeline
data_parallel_size = world_size // (tensor_model_parallel_size *
                                    pipeline_model_parallel_size) # 2
num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size # 8
num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size # 4
num_data_parallel_groups = world_size // data_parallel_size # 8

# Build the data-parallel groups.
print("------ Build the data-parallel groups -----")
all_data_parallel_group_ranks = []
for i in range(pipeline_model_parallel_size):
    start_rank = i * num_pipeline_model_parallel_groups
    end_rank = (i + 1) * num_pipeline_model_parallel_groups
    for j in range(tensor_model_parallel_size):
        ranks = range(start_rank + j, end_rank,
                      tensor_model_parallel_size)
        all_data_parallel_group_ranks.append(list(ranks))
print(all_data_parallel_group_ranks)

# Build the model-parallel groups.
print("------ Build the model-parallel groups -----")
for i in range(data_parallel_size):
    ranks = [data_parallel_group_ranks[i]
             for data_parallel_group_ranks in all_data_parallel_group_ranks]
    print(list(ranks))

# Build the tensor model-parallel groups.
print("------ Build the tensor model-parallel groups -----")
for i in range(num_tensor_model_parallel_groups):
    ranks = range(i * tensor_model_parallel_size,
                  (i + 1) * tensor_model_parallel_size)
    print(list(ranks))

# Build the pipeline model-parallel groups and embedding groups
# (first and last rank in each pipeline model-parallel group).
print("------ Build the pipeline model-parallel groups -----")
for i in range(num_pipeline_model_parallel_groups):
    ranks = range(i, world_size,
                  num_pipeline_model_parallel_groups)
    print(list(ranks))

è¾“å‡ºå¦‚ä¸‹ã€‚éœ€è¦æ³¨æ„ï¼Œè¿™é‡Œéƒ½æ˜¯ GPU çš„åºåˆ—å·ï¼Œ[0,2] å°±æ˜¯ [g0, g2]ï¼š

    ------ Build the data-parallel groups -----
    [[0, 2], [1, 3], [4, 6], [5, 7], [8, 10], [9, 11], [12, 14], [13, 15]]
    ------ Build the model-parallel groups -----
    [0, 1, 4, 5, 8, 9, 12, 13]
    [2, 3, 6, 7, 10, 11, 14, 15]
    ------ Build the tensor model-parallel groups -----
    [0, 1]
    [2, 3]
    [4, 5]
    [6, 7]
    [8, 9]
    [10, 11]
    [12, 13]
    [14, 15]
    ------ Build the pipeline model-parallel groups -----
    [0, 4, 8, 12]
    [1, 5, 9, 13]
    [2, 6, 10, 14]
    [3, 7, 11, 15]

æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹æ³¨é‡Šï¼Œå‘ç°ä»£ç æ‰“å°ç»“æœå¯ä»¥å’Œæ³¨é‡Šå¯¹åº”ä¸Šï¼š
    Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we
    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
    the model pipeline. The present function will
    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups
    and 8 data-parallel groups as:
        8 data_parallel groups:
            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
        8 tensor model-parallel groups:
            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
        4 pipeline model-parallel groups:
            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
æˆ‘ä»¬æ¥ä¸‹æ¥ä¼šè¿›è¡Œå…·ä½“åˆ†æã€‚


0x04 èµ·å§‹çŠ¶æ€
4.1 GPU çŠ¶å†µ
ä»æ³¨é‡Šä¸­å¯ä»¥çœ‹åˆ°ï¼š
    Note that for efficiency, the caller should make sure adjacent ranks are on the same DGX box.
    For example if we are using 2 DGX-1 boxes with a total of 16 GPUs,
    rank 0 to 7 belong to the first box and ranks 8 to 15 belong to the second box.
æ„æ€å°±æ˜¯ï¼šè°ƒç”¨è€…éœ€è¦ç¡®ä¿ç›¸é‚»çš„rankåœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œ
æˆ‘ä»¬ä¾‹å­æœ‰ä¸¤ä¸ªNodeï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªNodeæ‹¥æœ‰ GPU 0 ï½ 7ï¼Œå°±æ˜¯ rank 0 ï½ 7ï¼Œç¬¬äºŒä¸ªNodeæ˜¯ GPU 8ï½15ï¼Œå°±æ˜¯ rank 8 ï½ 15ã€‚
å…·ä½“å¦‚ä¸‹ï¼Œè¿™é‡Œæ¯è¡Œ4ä¸ªGPUï¼Œæ˜¯å› ä¸º 4 GPUs to parallelize the model pipelineï¼Œæ‰€ä»¥æµæ°´çº¿æ¯ä¸ªstageæ˜¯4ä¸ªGPUã€‚
    52.png

4.2 ç¬¦å·è¯´æ˜
ä¸‹é¢æ˜¯è®ºæ–‡ä¹‹ä¸­æåˆ°çš„ä¸€äº›ç¬¦å·ï¼Œè¿™é‡Œæœ‰å¿…è¦å†å–å‡ºæ¥æ¸©ä¹ ä¸€ä¸‹ï¼š

    (ğ‘, ğ‘¡, ğ‘‘): Parallelization dimensions.

    ğ‘ for the pipeline-modelparallel size,

    ğ‘¡ for the tensor-model-parallel size, and ğ‘‘ for the data-parallel size.

    ğ‘›: Number of GPUs. We require ğ‘ Â· ğ‘¡ Â· ğ‘‘ = ğ‘›.

4.3 åˆå§‹åˆ†ç»„
ä¾æ®æ³¨é‡Šï¼Œæˆ‘ä»¬å¾—å‡ºç›®å‰åˆ†ç»„æƒ…å†µå’Œä¸€äº›å…¨å±€ä¿¡æ¯ã€‚

    ä¸€å…±16ä¸ªGPUï¼Œæ‰€ä»¥ world_size ä¸º 16ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ nã€‚

    ä½¿ç”¨ä¸¤ä¸ªGPUè¿›è¡Œ model tensor å¹¶è¡Œï¼Œæ‰€ä»¥ tensor_model_parallel_size = 2ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ tã€‚

    ä½¿ç”¨å››ä¸ªGPUè¿›è¡Œæ¨¡å‹æµæ°´çº¿å¹¶è¡Œï¼Œæ‰€ä»¥ pipeline_model_parallel_size = 4ã€‚å°±æ˜¯ Notation ä¹‹ä¸­çš„ pã€‚
    å…¶å®ï¼Œå°±æ˜¯æµæ°´çº¿æ·±åº¦ä¸º 4ï¼Œå³ï¼Œ4 ä¸ª GPU æ˜¯ä¸²è¡Œçš„ã€‚

    ä¾æ®ä¸Šé¢å®šä¹‰ï¼Œd = n / ( t * p) = 2ï¼Œå°±æ˜¯ data_parallel_size = 2ã€‚
    å› ä¸º t * p å°±æ˜¯ä¸€ä¸ªæ¨¡å‹æ‰€éœ€è¦çš„ GPUï¼Œd = (æ€» GPU / ä¸€ä¸ªæ¨¡å‹éœ€è¦çš„ GPU)ï¼Œ
    ç»“æœæ˜¯è¿™äº›GPUå¯ä»¥è®­ç»ƒ d ä¸ªæ¨¡å‹ï¼Œå°±æ˜¯å¯ä»¥ç”¨ d ä¸ª mini-batches è¿›è¡Œè¿™ä¸ª dä¸ªæ¨¡å‹ä¸€èµ·è®­ç»ƒï¼Œæ‰€ä»¥æ•°æ®å¹¶è¡Œåº¦ä¸º dã€‚

æ¥ä¸‹æ¥ç»“åˆä»£ç çœ‹çœ‹éœ€è¦åˆ†æˆå¤šå°‘ä¸ªprocess groupsï¼Œä»–ä»¬åœ¨ä»£ç ä¹‹ä¸­çš„å˜é‡æ˜¯ä»€ä¹ˆã€‚

    num_tensor_model_parallel_groups å°±æ˜¯ä» tensor model å¹¶è¡Œè§’åº¦çœ‹ï¼Œåˆ†æˆ8 ä¸ªè¿›ç¨‹roupã€‚

    num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size å°±æ˜¯ä» model å¹¶è¡Œè§’åº¦çœ‹ï¼Œ
    åˆ†æˆ 4 ä¸ª è¿›ç¨‹groupã€‚

    num_data_parallel_groups = world_size // data_parallel_size å°±æ˜¯ä»data å¹¶è¡Œè§’åº¦çœ‹ï¼Œ
    åˆ†æˆ8 ä¸ª è¿›ç¨‹groupã€‚å°±æ˜¯ä¼šæœ‰ 8 ä¸ª DDPï¼Œæ¯ä¸ª DDP åŒ…æ‹¬ 2 ä¸ª rankã€‚

    è¿˜æœ‰ä¸€ä¸ª _MODEL_PARALLEL_GROUPï¼Œ

å…·ä½“å¦‚ä¸‹ï¼š

    world_size = 16
    tensor_model_parallel_size = 2 # 2 GPUs to parallelize the model tensor
    pipeline_model_parallel_size = 4 # 4 GPUs to parallelize the model pipeline
    data_parallel_size = world_size // (tensor_model_parallel_size *
                                        pipeline_model_parallel_size) # 2
    num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size # 8
    num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size # 4
    num_data_parallel_groups = world_size // data_parallel_size # 8



0x05 Tensor model-parallel
æœ¬èŠ‚æˆ‘ä»¬åˆ†æçš„æ˜¯ï¼Œå¦‚ä½•å°† Node ä¸Šçš„ GPU åˆ†ç»™ tensor model å¹¶è¡Œç»„ã€‚

5.1 åˆ†ç»„
å¯¹äºæ³¨é‡Šä¾‹å­ï¼Œ16 / 2 = 8ï¼Œåˆ†æˆ 8 ä¸ªè¿›ç¨‹ç»„ï¼Œæ¯ä¸ªç»„ ä¸¤ä¸ª rankã€‚
è¿™äº›åˆ†ç»„åˆ†åˆ«æ˜¯ï¼š[g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]ï¼Œ
æˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹ä¿¡æ¯ï¼š

    [g0, g1] å°±æ˜¯æŸä¸€å±‚åˆ†åˆ‡ä¸º2åŠï¼Œåˆ†åˆ«è¢« g0, g1 æ¥æ‰§è¡Œï¼Œ[g2, g3] è¡¨ç¤ºå¦ä¸€å±‚è¢«åˆ†ä¸ºä¸¤å±‚ï¼Œåˆ†åˆ«è¢« g2ï¼Œg3 æ¥æ‰§è¡Œã€‚

    æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€ä¸ª tensor-model-parallel groupçš„ rankä¸€å®šæ˜¯ç›¸é‚»çš„ï¼Œæ¯”å¦‚ [g0, g1], [g2, g3]ã€‚

    æ³¨æ„ï¼Œ0 ~ 7 ä¸ä»£è¡¨æ˜¯åŒä¸€ä¸ªæ¨¡å‹ã€‚0 ~ 7 æ˜¯åŒä¸€ä¸ª Node ä¸Šçš„ GPUï¼Œè¿™ç‚¹å®¹æ˜“è¢«æ··æ·†ã€‚

æˆ‘ä»¬å†çœ‹çœ‹ä»£ç ï¼š

    # Build the tensor model-parallel groups.
    global _TENSOR_MODEL_PARALLEL_GROUP
    for i in range(num_tensor_model_parallel_groups): # 8
        ranks = range(i * tensor_model_parallel_size,
                      (i + 1) * tensor_model_parallel_size)
        group = torch.distributed.new_group(ranks) # å°±æœ‰ç”Ÿæˆ 8 ç»„
        if rank in ranks:
            # å¦‚æœæœ¬rankåœ¨æŸä¸€listä¹‹ä¸­ï¼Œå³1 åœ¨ [0,1] ä¹‹ä¸­ï¼Œåˆ™æœ¬ rank å°±å±äº new_group([0,1])
            _TENSOR_MODEL_PARALLEL_GROUP = group
æˆ‘ä»¬å®éªŒä¹‹ä¸­åœ¨è¿™é‡Œå¾—åˆ°ï¼š

    ------ Build the tensor model-parallel groups -----
    [0, 1]
    [2, 3]
    [4, 5]
    [6, 7]
    [8, 9]
    [10, 11]
    [12, 13]
    [14, 15]

å¯¹åº”æˆ‘ä»¬å›¾ä¸Šå¦‚ä¸‹ï¼Œæ¯ä¸ª tensor model group ç”¨ä¸€ä¸ªè™šçº¿å°çŸ©å½¢æ¡†æ ‡ç¤ºï¼Œä¸€å…±8ä¸ªï¼š
53.jpg

_TENSOR_MODEL_PARALLEL_GROUP = group å°±è®°å½•äº†æœ¬rankçš„è¿›ç¨‹ç»„ä¿¡æ¯ï¼Œ
æ¯”å¦‚ rank 2ï¼Œå®ƒçš„ _TENSOR_MODEL_PARALLEL_GROUP å†…å®¹å°±æ˜¯ï¼šgroup([g2, g3])ã€‚

5.2 ä½¿ç”¨
æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ã€‚

    get_tensor_model_parallel_group è¿”å›äº†è‡ªå·± rank å¯¹åº”çš„ tensor model groupã€‚

    def get_tensor_model_parallel_group():
        """Get the tensor model parallel group the caller rank belongs to."""
        return _TENSOR_MODEL_PARALLEL_GROUP

åœ¨ megatron/mpu/mappings.py ä¹‹ä¸­æœ‰å¯¹ tensor model group çš„ä½¿ç”¨ï¼š

    def _reduce(input_):
        """All-reduce the input tensor across model parallel group."""

        # Bypass the function if we are using only 1 GPU.
        if get_tensor_model_parallel_world_size()==1:
            return input_

        # All-reduce.
        torch.distributed.all_reduce(input_, group=get_tensor_model_parallel_group())

        return input_

å°±æ˜¯å½“æµæ°´çº¿åå‘ä¼ æ’­æ—¶å€™ï¼Œåˆ©ç”¨ _TENSOR_MODEL_PARALLEL_GROUP è¿›è¡Œåœ¨ç»„å†…è¿›è¡Œé›†åˆé€šä¿¡ã€‚


0x06 Pipe-parallel
æœ¬èŠ‚æˆ‘ä»¬åˆ†æçš„æ˜¯ï¼Œå¦‚ä½•å°† Node ä¸Šçš„ GPU åˆ†ç»™ pipeline model å¹¶è¡Œç»„ã€‚

6.1 åˆ†ç»„
ä»æ³¨é‡Šå¯ä»¥çœ‹åˆ°ï¼Œæµæ°´çº¿åˆ†ç»„å°±æ˜¯æŠŠè¿™ä¸ª16ä¸ªGPU åˆ†æˆ 4 ç»„ï¼Œæ¯ç»„ 4 ä¸ª GPUï¼Œ
å¾—åˆ° [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹ä¿¡æ¯ï¼š

    æ¯ç»„çš„å››ä¸ªGPUè¿›è¡Œæ¨¡å‹æµæ°´çº¿å¹¶è¡Œï¼Œæ‰€ä»¥ pipeline_model_parallel_size = 4ã€‚
    å°±æ˜¯ Notation ä¹‹ä¸­çš„ pã€‚å…¶å®ï¼Œå°±æ˜¯æµæ°´çº¿æ·±åº¦ä¸º 4ï¼Œ æ¯ç»„å†… 4 ä¸ª GPU æ˜¯ä¸²è¡Œçš„ã€‚
    å³ï¼Œ [g0, g4, g8, g12] è¿™4ä¸ª GPUæ˜¯ä¸²è¡Œçš„ã€‚

    å†çœ‹çœ‹æµæ°´çº¿çš„æ¯ä¸€å±‚ï¼Œå«æœ‰ 16 / 4 = 4 ä¸ª GPUï¼Œèƒ½çœ‹åˆ°ç¬¬ä¸€å±‚æ˜¯ 0 ~ 4ï¼Œç¬¬äºŒå±‚æ˜¯ 5 ~ 8ï¼Œ......ã€‚

    å¯ä»¥çœ‹åˆ°ï¼Œæµæ°´çº¿çš„ groupæ˜¯éš” n // pä¸ªå–ä¸€ä¸ªï¼Œæ¯”å¦‚[0, 4, 8, 12]ã€‚

    å¯¹äºæµæ°´çº¿æ¯ä¸ªstageï¼Œåˆ™æ˜¯stage i çš„ rank èŒƒå›´æ˜¯ï¼š[(i-1) * n//p, (i) * n//p]ï¼Œ
    å³ rank 2 æ‰€åœ¨çš„stage çš„rankæ˜¯ [0,1,2,3]ã€‚

    _PIPELINE_MODEL_PARALLEL_GROUP å¾—åˆ°äº†æœ¬rankå¯¹åº”çš„æµæ°´çº¿è¿›ç¨‹ç»„ã€‚

    _PIPELINE_GLOBAL_RANKS å¾—åˆ°äº†è¿›ç¨‹ç»„çš„ranksã€‚

    å‡å¦‚æœ¬è¿›ç¨‹æ˜¯ rank 2ï¼Œåˆ™æµæ°´çº¿è¿›ç¨‹ç»„ ranks æ˜¯ [g2, g6, g10, g14]ã€‚

å…·ä½“ä»£ç å¦‚ä¸‹ï¼š

    # Build the pipeline model-parallel groups and embedding groups
    # (first and last rank in each pipeline model-parallel group).
    global _PIPELINE_MODEL_PARALLEL_GROUP
    global _PIPELINE_GLOBAL_RANKS
    global _EMBEDDING_GROUP
    for i in range(num_pipeline_model_parallel_groups): # 4
        ranks = range(i, world_size, # æ¯éš” n // pä¸ªå–ä¸€ä¸ª
                      num_pipeline_model_parallel_groups)
        group = torch.distributed.new_group(ranks)
        if rank in ranks:
            _PIPELINE_MODEL_PARALLEL_GROUP = group
            _PIPELINE_GLOBAL_RANKS = ranks
        # Setup embedding group (to exchange gradients between
        # first and last stages).
        if len(ranks) > 1:
            embedding_ranks = [ranks[0], ranks[-1]]
        else:
            embedding_ranks = ranks
        group = torch.distributed.new_group(embedding_ranks)
        if rank in embedding_ranks:
            _EMBEDDING_GROUP = group

æˆ‘ä»¬æ‹“å±•ä¹‹å‰å›¾å¦‚ä¸‹ï¼Œç°åœ¨çœ‹åˆ°å¢åŠ äº† 4 æ¡ä»ä¸Šåˆ°ä¸‹çš„è™šçº¿ç®­å¤´ï¼Œåˆ†åˆ«å¯¹åº”äº† 4 ç»„æµæ°´çº¿ä¸²è¡Œã€‚
æ¨ªå‘å±‚æ˜¯ä» Stage 0 ~ Stage 3ã€‚
    54.jpg

.............


0xFF å‚è€ƒ
[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(2)

[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(1)

Megatron-LMæºç é˜…è¯»ï¼ˆä¸€ï¼‰

Megatron-LMæºç é˜…è¯»ï¼ˆäºŒï¼‰

megatronå­¦ä¹ æ€»ç»“

GTC 2020: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

å¦‚ä½•è¯„ä»· NVIDIA å‘å¸ƒçš„ DGX-1ï¼Ÿ